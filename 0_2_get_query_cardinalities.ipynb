{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import math\n",
    "# from wanderjoin import WanderJoin\n",
    "from query_representation.query import parse_sql\n",
    "import re\n",
    "from networkx.readwrite import json_graph\n",
    "import pickle\n",
    "import json\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import klepto\n",
    "import random\n",
    "import pdb\n",
    "from query_representation.query import *\n",
    "from query_representation.utils import *\n",
    "import glob\n",
    "import psycopg2 as pg\n",
    "import argparse\n",
    "import sys\n",
    "sys.path.append(\".\")\n",
    "\n",
    "\n",
    "TIMEOUT_COUNT_CONSTANT = 150001000001\n",
    "CROSS_JOIN_CONSTANT = 150001000000\n",
    "EXCEPTION_COUNT_CONSTANT = 150001000002\n",
    "RERUN_TIMEOUTS = 1\n",
    "\n",
    "CACHE_TIMEOUT = 4\n",
    "CACHE_CARD_TYPES = [\"actual\"]\n",
    "\n",
    "DEBUG_CHECK_TIMES = False\n",
    "CONF_ALPHA = 0.99\n",
    "\n",
    "\n",
    "def pg_est_from_explain(output):\n",
    "    '''\n",
    "    '''\n",
    "    est_vals = None\n",
    "    for line in output:\n",
    "        line = line[0]\n",
    "        # getting estimate from the first Join, or Scan operator we see\n",
    "        if \"Seq Scan\" in line or \"Loop\" in line or \"Join\" in line \\\n",
    "                or \"Index Scan\" in line or \"Scan\" in line:\n",
    "            for w in line.split():\n",
    "                if \"rows\" in w and est_vals is None:\n",
    "                    est_vals = int(re.findall(\"\\d+\", w)[0])\n",
    "                    return est_vals\n",
    "\n",
    "    print(\"pg est failed!\")\n",
    "    print(output)\n",
    "    pdb.set_trace()\n",
    "    return 1.00\n",
    "\n",
    "\n",
    "def read_flags():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\"--db_name\", type=str, required=False,\n",
    "                        default=\"imdb\")\n",
    "    parser.add_argument(\"--db_host\", type=str, required=False,\n",
    "                        default=\"localhost\")\n",
    "    parser.add_argument(\"--user\", type=str, required=False,\n",
    "                        default=\"postgres\")\n",
    "    parser.add_argument(\"--pwd\", type=str, required=False,\n",
    "                        default=\"postgres\")\n",
    "    parser.add_argument(\"--card_cache_dir\", type=str, required=False,\n",
    "                        default=\"./cardinality_cache\")\n",
    "    parser.add_argument(\"--port\", type=str, required=False,\n",
    "                        default=5432)\n",
    "    parser.add_argument(\"--wj_walk_timeout\", type=float, required=False,\n",
    "                        default=0.5)\n",
    "    # \n",
    "    parser.add_argument(\"--query_dir\", type=str, required=False,\n",
    "                        default=\"./imdb-new-workload/multi_column_3/\")\n",
    "    parser.add_argument(\"-n\", \"--num_queries\", type=int,\n",
    "                        required=False, default=5)\n",
    "    parser.add_argument(\"--use_tries\", type=int,\n",
    "                        required=False, default=1)\n",
    "    parser.add_argument(\"--skip_zero_queries\", type=int,\n",
    "                        required=False, default=1)\n",
    "    parser.add_argument(\"--no_parallel\", type=int,\n",
    "                        required=False, default=0)\n",
    "    #\n",
    "    parser.add_argument(\"--card_type\", type=str, required=False,\n",
    "                        default=\"pg\")\n",
    "    #\n",
    "    parser.add_argument(\"--key_name\", type=str, required=False,\n",
    "                        default=\"expected\")\n",
    "    parser.add_argument(\"--true_timeout\", type=int,\n",
    "                        required=False, default=1800000*5)\n",
    "    parser.add_argument(\"--pg_total\", type=int,\n",
    "                        required=False, default=1)\n",
    "    parser.add_argument(\"--num_proc\", type=int,\n",
    "                        required=False, default=-1)\n",
    "    parser.add_argument(\"--seed\", type=int,\n",
    "                        required=False, default=1234)\n",
    "    parser.add_argument(\"--sampling_percentage\", type=int,\n",
    "                        required=False, default=None)\n",
    "    parser.add_argument(\"--sampling_type\", type=str,\n",
    "                        required=False, default=None)\n",
    "    parser.add_argument(\"--db_year\", type=int,\n",
    "                        required=False, default=None)\n",
    "\n",
    "    return parser.parse_args([])\n",
    "\n",
    "\n",
    "def update_bad_qrep(qrep):\n",
    "    qrep = parse_sql(qrep[\"sql\"], None, None, None, None, None,\n",
    "                     compute_ground_truth=False)\n",
    "    qrep[\"subset_graph\"] = \\\n",
    "        nx.OrderedDiGraph(json_graph.adjacency_graph(qrep[\"subset_graph\"]))\n",
    "    qrep[\"join_graph\"] = json_graph.adjacency_graph(qrep[\"join_graph\"])\n",
    "    return qrep\n",
    "\n",
    "\n",
    "def is_cross_join(sg):\n",
    "    '''\n",
    "    enforces the constraint that the graph should be connected.\n",
    "    '''\n",
    "    if len(sg.nodes()) < 2:\n",
    "        # FIXME: should be return False\n",
    "        return False\n",
    "    sg2 = nx.Graph(sg)\n",
    "    to_remove = []\n",
    "\n",
    "    # do this in case of stackexchange database; because of the weird query\n",
    "    # structure, if the graph is connected only through the `site` table, it\n",
    "    # still behaves like a cross-join. Check the appendix of the Flow-Loss\n",
    "    # paper for more details\n",
    "    for node, data in sg2.nodes(data=True):\n",
    "        if data[\"real_name\"] == \"site\":\n",
    "            to_remove.append(node)\n",
    "\n",
    "    for node in to_remove:\n",
    "        sg2.remove_node(node)\n",
    "    if nx.is_connected(sg2):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def get_cardinality_wj(qrep, card_type, key_name, db_host, db_name, user, pwd,\n",
    "                       port, fn, wj_fn, wj_walk_timeout, idx, seed, trie_cache, use_tries):\n",
    "\n",
    "    key_name = \"wanderjoin-\" + str(wj_walk_timeout)\n",
    "    # key_name = \"wj\" + str(wj_walk_timeout)\n",
    "    for subset, info in qrep[\"subset_graph\"].nodes().items():\n",
    "        cards = info[\"cardinality\"]\n",
    "        if key_name in cards:\n",
    "            return\n",
    "        if \"actual\" not in cards:\n",
    "            return\n",
    "\n",
    "    if idx % 10 == 0:\n",
    "        print(\"query: \", idx)\n",
    "    start = time.time()\n",
    "    wj = WanderJoin(user, pwd, db_host, port,\n",
    "                    db_name, verbose=True, walks_timeout=wj_walk_timeout, seed=seed, use_tries=use_tries, trie_cache=trie_cache)\n",
    "\n",
    "    if SOURCE_NODE in list(qrep[\"subset_graph\"].nodes()):\n",
    "        qrep[\"subset_graph\"].remove_node(SOURCE_NODE)\n",
    "\n",
    "    data = wj.get_counts(qrep)\n",
    "\n",
    "    # save wj data\n",
    "    for subset, info in qrep[\"subset_graph\"].nodes().items():\n",
    "        cards = info[\"cardinality\"]\n",
    "        num = data[\"card_samples\"][subset]\n",
    "        est = math.ceil(data[\"card_ests_sum\"][subset] / num)\n",
    "        if num <= 1:\n",
    "            std = 0\n",
    "        else:\n",
    "            std = np.sqrt(data[\"card_vars_sum\"][subset] / float(num-1))\n",
    "        alpha = st.norm.ppf((CONF_ALPHA+1)/2)\n",
    "        half_interval = std*alpha / np.sqrt(num)\n",
    "\n",
    "        cards[key_name] = est\n",
    "        cards[key_name + \"_half_interval\"] = half_interval\n",
    "\n",
    "    old_data = load_object(wj_fn)\n",
    "    if old_data is None:\n",
    "        old_data = {}\n",
    "    old_data[key_name] = data\n",
    "\n",
    "    save_qrep(fn, qrep)\n",
    "    save_object(wj_fn, old_data)\n",
    "    print(\"wanderjoin, seed: {} took {}\".format(seed, time.time()-start))\n",
    "    return qrep\n",
    "\n",
    "\n",
    "def get_cardinality(qrep, card_type, key_name, db_host, db_name, user, pwd,\n",
    "                    port, true_timeout, pg_total, cache_dir, fn, wj_walk_timeout, idx,\n",
    "                    sampling_percentage, sampling_type, skip_zero_queries, db_year):\n",
    "    '''\n",
    "    updates qrep's fields with the needed cardinality estimates, and returns\n",
    "    the qrep.\n",
    "    '''\n",
    "    print(\"get cardinality!\")\n",
    "\n",
    "    # key_name不会是None\n",
    "    if key_name is None:\n",
    "        key_name = card_type\n",
    "\n",
    "    if db_year is not None:\n",
    "        db_name = db_name + str(db_year)\n",
    "\n",
    "    if sampling_percentage is not None:\n",
    "        key_name = str(sampling_type) + \\\n",
    "            str(sampling_percentage) + \"_\" + key_name\n",
    "\n",
    "        con = pg.connect(user=user, host=db_host, port=port,\n",
    "                         password=pwd, database=db_name)\n",
    "\n",
    "        cursor = con.cursor()\n",
    "\n",
    "    # if idx % 10 == 0:\n",
    "    #     print(\"query: \", idx)\n",
    "    print(\"query: \", idx)\n",
    "\n",
    "    # 不进入\n",
    "    # load the cache for few types\n",
    "    if card_type in CACHE_CARD_TYPES:\n",
    "        sql_cache = klepto.archives.dir_archive(cache_dir,\n",
    "                                                cached=True, serialized=True)\n",
    "    found_in_cache = 0\n",
    "    existing = 0\n",
    "    num_timeout = 0\n",
    "    site_cj = 0\n",
    "    query_exec_times = []\n",
    "\n",
    "    node_list = list(qrep[\"subset_graph\"].nodes())\n",
    "    node_list.sort(reverse=True, key=lambda x: len(x))\n",
    "    if args.db_name == \"so\":\n",
    "        source_node = tuple([\"SOURCE\"])\n",
    "        if source_node in node_list:\n",
    "            node_list.remove(source_node)\n",
    "    elif args.db_name == \"imdb\":\n",
    "        source_node = tuple([\"s\"])\n",
    "        if source_node in node_list:\n",
    "            node_list.remove(source_node)\n",
    "\n",
    "    card_key = \"cardinality\"\n",
    "    if db_year is not None:\n",
    "        card_key = str(db_year) + card_key\n",
    "\n",
    "    for subqi, subset in enumerate(node_list):\n",
    "        info = qrep[\"subset_graph\"].nodes()[subset]\n",
    "        if card_key not in info:\n",
    "            info[card_key] = {}\n",
    "\n",
    "        if \"exec_time\" not in info:\n",
    "            info[\"exec_time\"] = {}\n",
    "\n",
    "        cards = info[card_key]\n",
    "        execs = info[\"exec_time\"]\n",
    "        sg = qrep[\"join_graph\"].subgraph(subset)\n",
    "        subsql = nx_graph_to_query(sg)\n",
    "\n",
    "        if sampling_percentage is not None:\n",
    "            table_names = []\n",
    "            for k, v in sg.nodes(data=True):\n",
    "                table = v[\"real_name\"]\n",
    "                new_table_name = table + \"_\" + \\\n",
    "                    sampling_type + str(sampling_percentage)\n",
    "                new_table_name += \" \"\n",
    "                new_table_name = \" \" + new_table_name\n",
    "                # TODO: check if table exists...\n",
    "                cursor.execute(\n",
    "                    \"select * from information_schema.tables where table_name='{}'\".format(new_table_name))\n",
    "                if bool(cursor.rowcount):\n",
    "                    subsql = re.sub(r\"\\b {} \\b\".format(table), new_table_name,\n",
    "                                    subsql)\n",
    "\n",
    "        if key_name in cards \\\n",
    "                and not DEBUG_CHECK_TIMES:\n",
    "            if key_name == \"actual\":\n",
    "                if cards[key_name] == 0 and skip_zero_queries:\n",
    "                    # don't want to get cardinalities for zero queries\n",
    "                    break\n",
    "\n",
    "                elif cards[key_name] >= TIMEOUT_COUNT_CONSTANT and not RERUN_TIMEOUTS:\n",
    "                    existing += 1\n",
    "                    continue\n",
    "\n",
    "                elif cards[key_name] == EXCEPTION_COUNT_CONSTANT:\n",
    "                    existing += 1\n",
    "                    continue\n",
    "\n",
    "                elif cards[key_name] < TIMEOUT_COUNT_CONSTANT:\n",
    "                    existing += 1\n",
    "                    continue\n",
    "\n",
    "                print(\"key existing: {}, but going to rerun\".format(\n",
    "                    cards[key_name]))\n",
    "\n",
    "            # TODO: not sure why this here\n",
    "            # if not (sampling_percentage is not None and \\\n",
    "                # cards[key_name] >= TIMEOUT_COUNT_CONSTANT):\n",
    "                # existing += 1\n",
    "                # continue\n",
    "\n",
    "        if card_type == \"pg\":\n",
    "            subsql = \"EXPLAIN \" + subsql\n",
    "            output = execute_query(subsql, user, db_host,\n",
    "                                   port, pwd, db_name, [])\n",
    "            card = pg_est_from_explain(output)\n",
    "            cards[key_name] = card\n",
    "            if subqi % 10 == 0:\n",
    "                save_qrep(fn, qrep)\n",
    "\n",
    "        elif card_type == \"actual\":\n",
    "            if subqi % 10 == 0:\n",
    "                save_qrep(fn, qrep)\n",
    "\n",
    "            hash_sql = deterministic_hash(subsql)\n",
    "            if \"count\" not in subsql.lower():\n",
    "                print(\"cardinality query does not have count\")\n",
    "                pdb.set_trace()\n",
    "            if is_cross_join(sg):\n",
    "                site_cj += 1\n",
    "                card = CROSS_JOIN_CONSTANT\n",
    "                cards[key_name] = card\n",
    "                continue\n",
    "\n",
    "            if hash_sql in sql_cache.archive \\\n",
    "                    and not DEBUG_CHECK_TIMES:\n",
    "                card = sql_cache.archive[hash_sql]\n",
    "                found_in_cache += 1\n",
    "                cards[key_name] = card\n",
    "                continue\n",
    "\n",
    "            start = time.time()\n",
    "            pre_execs = [\"SET statement_timeout = {}\".format(true_timeout)]\n",
    "            output = execute_query(subsql, user, db_host, port, pwd, db_name,\n",
    "                                   pre_execs)\n",
    "            if isinstance(output, Exception):\n",
    "                print(output)\n",
    "                card = EXCEPTION_COUNT_CONSTANT\n",
    "                num_timeout += 1\n",
    "                # continue\n",
    "                # pdb.set_trace()\n",
    "            elif output == \"timeout\":\n",
    "                print(\"timeout query: \")\n",
    "                print(subsql)\n",
    "                card = TIMEOUT_COUNT_CONSTANT\n",
    "                num_timeout += 1\n",
    "            else:\n",
    "                card = output[0][0]\n",
    "\n",
    "            exec_time = time.time() - start\n",
    "            if exec_time > CACHE_TIMEOUT:\n",
    "                print(exec_time)\n",
    "                sql_cache.archive[hash_sql] = card\n",
    "            cards[key_name] = card\n",
    "            execs[key_name] = exec_time\n",
    "            query_exec_times.append(exec_time)\n",
    "            if card == 0 and skip_zero_queries:\n",
    "                # bad times...\n",
    "                print(\"skipping query with zero cardinality subquery\")\n",
    "                break\n",
    "\n",
    "        # elif card_type == \"wanderjoin\":\n",
    "        #     assert \"SELECT\" in subsql\n",
    "        #     subsql = subsql.replace(\"SELECT\", \"SELECT ONLINE\")\n",
    "        #     subsql = subsql.replace(\";\", \"\")\n",
    "        #     subsql += WANDERJOIN_TIME_FMT.format(\n",
    "        #         TIME=wj_walk_timeout,\n",
    "        #         CONF=95,\n",
    "        #         INT=1000)\n",
    "        #     print(subsql)\n",
    "        #     output = execute_query(subsql, user, db_host, port, pwd, db_name,\n",
    "        #                            [])\n",
    "        #     print(output)\n",
    "        #     pdb.set_trace()\n",
    "        #     assert False\n",
    "\n",
    "        elif card_type == \"total\":\n",
    "            exec_sql = get_total_count_query(subsql)\n",
    "            if args.pg_total:\n",
    "                exec_sql = \"EXPLAIN \" + exec_sql\n",
    "\n",
    "            output = execute_query(\n",
    "                exec_sql, user, db_host, port, pwd, db_name, [])\n",
    "            card = pg_est_from_explain(output)\n",
    "            cards[key_name] = card\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "    if card_type == \"actual\":\n",
    "        print(\"total: {}, timeout: {}, existing: {}, found in cache: {}\".format(\n",
    "            len(qrep[\"subset_graph\"].nodes()), num_timeout, existing, found_in_cache))\n",
    "        # print(\"site cj: \", site_cj)\n",
    "        if len(query_exec_times) != 0:\n",
    "            print(\"avg exec time: \", sum(\n",
    "                query_exec_times) / len(query_exec_times))\n",
    "\n",
    "    if fn is not None:\n",
    "        # update_qrep(qrep)\n",
    "        save_qrep(fn, qrep)\n",
    "        print(\"updated sql rep!\")\n",
    "\n",
    "    sys.stdout.flush()\n",
    "    return qrep\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    fns = list(glob.glob(args.query_dir + \"/*\"))\n",
    "    fns.sort()\n",
    "    par_args = []\n",
    "\n",
    "    for i, fn in enumerate(fns):\n",
    "        if i >= args.num_queries and args.num_queries != -1:\n",
    "            break\n",
    "\n",
    "        if (\".pkl\" not in fn and \".sql\" not in fn):\n",
    "            continue\n",
    "\n",
    "        # 判断是否已生成pkl文件，如果已生成pkl文件,则直接读取；否则，和sql_to_qrep.py中一样的步骤生成pkl文件\n",
    "        if \".pkl\" in fn:\n",
    "            qrep = load_qrep(fn)\n",
    "        else:\n",
    "            with open(fn, \"r\") as f:\n",
    "                sql = f.read()\n",
    "            sql = sql.strip()\n",
    "            if \"SELECT\" not in sql:\n",
    "                continue\n",
    "\n",
    "            qrep = parse_sql(sql, None, None, None, None, None,\n",
    "                             compute_ground_truth=False)\n",
    "\n",
    "            qrep[\"subset_graph\"] = \\\n",
    "                nx.OrderedDiGraph(\n",
    "                    json_graph.adjacency_graph(qrep[\"subset_graph\"]))\n",
    "            qrep[\"join_graph\"] = json_graph.adjacency_graph(qrep[\"join_graph\"])\n",
    "            fn = fn.replace(\".sql\", \".pkl\")\n",
    "            save_qrep(fn, qrep)\n",
    "            print(\"updated sql rep!\")\n",
    "            continue\n",
    "\n",
    "        # no_parallel默认为false，不进入if\n",
    "        if args.no_parallel:\n",
    "            if args.card_type == \"wanderjoin\":\n",
    "                wj_dir = os.path.dirname(fn) + \"/wj_data/\"\n",
    "                base_name = os.path.basename(fn)\n",
    "                if not os.path.exists(wj_dir):\n",
    "                    make_dir(wj_dir)\n",
    "                wj_fn = wj_dir + base_name\n",
    "                get_cardinality_wj(qrep, args.card_type, args.key_name, args.db_host,\n",
    "                                   args.db_name, args.user, args.pwd, args.port,\n",
    "                                   fn, wj_fn, args.wj_walk_timeout, i, args.seed, None,\n",
    "                                   args.use_tries)\n",
    "                print(\"done!\")\n",
    "                pdb.set_trace()\n",
    "            else:\n",
    "                get_cardinality(qrep, args.card_type, args.key_name, args.db_host,\n",
    "                                args.db_name, args.user, args.pwd, args.port,\n",
    "                                args.true_timeout, args.pg_total, args.card_cache_dir, fn,\n",
    "                                args.wj_walk_timeout, i, args.sampling_percentage,\n",
    "                                args.sampling_type, True, args.db_year)\n",
    "\n",
    "            continue\n",
    "\n",
    "        # 仓库中命令行card_type=\"pg\"，不进入if\n",
    "        if args.card_type == \"wanderjoin\":\n",
    "            par_func = get_cardinality_wj\n",
    "\n",
    "            wj_dir = os.path.dirname(fn) + \"/wj_data/\"\n",
    "            base_name = os.path.basename(fn)\n",
    "            if not os.path.exists(wj_dir):\n",
    "                make_dir(wj_dir)\n",
    "            wj_fn = wj_dir + base_name\n",
    "            # trie_cache = klepto.archives.dir_archive(\"./trie_cache\",\n",
    "            # cached=True, serialized=True)\n",
    "            # tstart = time.time()\n",
    "            # print(\"going to load trie archive...\")\n",
    "            # trie_cache.load()\n",
    "            # print(\"loading trie archive took: \", time.time() - tstart)\n",
    "            par_args.append((qrep, args.card_type, args.key_name, args.db_host,\n",
    "                             args.db_name, args.user, args.pwd, args.port,\n",
    "                             fn, wj_fn, args.wj_walk_timeout, i, args.seed, None,\n",
    "                             args.use_tries))\n",
    "        # 进入else\n",
    "        else:\n",
    "\n",
    "            # 尝试不并行\n",
    "            # qrep = get_cardinality(qrep, args.card_type, args.key_name, args.db_host,\n",
    "            #                        args.db_name, args.user, args.pwd, args.port,\n",
    "            #                        args.true_timeout, args.pg_total, args.card_cache_dir, fn,\n",
    "            #                        args.wj_walk_timeout, i, args.sampling_percentage,\n",
    "            #                        args.sampling_type, args.skip_zero_queries, args.db_year)\n",
    "\n",
    "            par_func = get_cardinality\n",
    "            par_args.append((qrep, args.card_type, args.key_name, args.db_host,\n",
    "                             args.db_name, args.user, args.pwd, args.port,\n",
    "                             args.true_timeout, args.pg_total, args.card_cache_dir, fn,\n",
    "                             args.wj_walk_timeout, i, args.sampling_percentage,\n",
    "                             args.sampling_type, args.skip_zero_queries, args.db_year))\n",
    "\n",
    "    if args.no_parallel:\n",
    "        print(\"Generated all cardinalities\")\n",
    "        exit(1)\n",
    "\n",
    "    start = time.time()\n",
    "    if args.num_proc == -1:\n",
    "        num_proc = cpu_count()\n",
    "    else:\n",
    "        num_proc = args.num_proc\n",
    "    print(\"num_proc is \", num_proc)\n",
    "    print(\"args is \", args)\n",
    "    # 尝试注释\n",
    "    with Pool(processes=num_proc) as pool:\n",
    "        # print(\"par_func is \", par_func, \"\\npar_args is \", par_args)\n",
    "        qreps = pool.starmap(par_func, par_args)\n",
    "    print(\"Generated all cardinalities in {} seconds\".format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_proc is  1\n",
      "args is  Namespace(db_name='imdb', db_host='localhost', user='postgres', pwd='postgres', card_cache_dir='./cardinality_cache', port=5432, wj_walk_timeout=0.5, query_dir='./imdb-new-workload/multi_column_3/', num_queries=5, use_tries=1, skip_zero_queries=1, no_parallel=0, card_type='pg', key_name='expected', true_timeout=9000000, pg_total=1, num_proc=-1, seed=1234, sampling_percentage=None, sampling_type=None, db_year=None)\n",
      "par_func is  <function get_cardinality at 0x0000018CB44A9DC0> \n",
      "par_args is  [({'sql': \"SELECT COUNT(*) FROM title as t,\\nkind_type as kt,\\ninfo_type as it1,\\nmovie_info as mi1,\\ncast_info as ci,\\nrole_type as rt,\\nname as n,\\nmovie_keyword as mk,\\nkeyword as k,\\nmovie_companies as mc,\\ncompany_type as ct,\\ncompany_name as cn\\nWHERE\\nt.id = ci.movie_id\\nAND t.id = mc.movie_id\\nAND t.id = mi1.movie_id\\nAND t.id = mk.movie_id\\nAND mc.company_type_id = ct.id\\nAND mc.company_id = cn.id\\nAND k.id = mk.keyword_id\\nAND mi1.info_type_id = it1.id\\nAND t.kind_id = kt.id\\nAND ci.person_id = n.id\\nAND ci.role_id = rt.id\\nAND (it1.id IN ('1'))\\nAND (mi1.info in ('30','60','USA:30','USA:60'))\\nAND (kt.kind in ('episode'))\\nAND (rt.role in ('actor','actress','director'))\\nAND (n.gender in ('f','m'))\\nAND (n.name_pcode_nf in ('A4163','C6426','D1316','D5216','G6216','G6252','J525','M6263','P3616','P3624','P436','W4525'))\\nAND (t.production_year <= 1975)\\nAND (t.production_year >= 1925)\\nAND (cn.name in ('American Broadcasting Company (ABC)','Columbia Broadcasting System (CBS)','National Broadcasting Company (NBC)'))\\nAND (ct.kind in ('distributors'))\\n\", 'join_graph': <networkx.classes.graph.Graph object at 0x0000018C94B7A130>, 'subset_graph': <networkx.classes.ordered.OrderedDiGraph object at 0x0000018C94B87FD0>}, 'pg', 'expected', 'localhost', 'imdb', 'postgres', 'postgres', 5432, 9000000, 1, './cardinality_cache', './imdb-new-workload/multi_column_3\\\\8a506.pkl', 0.5, 0, None, None, 1, None)]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    args = read_flags()\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
